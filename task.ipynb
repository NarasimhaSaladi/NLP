{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "corpus = [\n",
    "    \"Natural Language Processing is an exciting field.\",\n",
    "    \"Preprocessing techniques are crucial for NLP tasks.\",\n",
    "    \"One Hot Encoding is a binary representation method.\",\n",
    "    \"Bag of Words is a common technique in NLP.\",\n",
    "    \"Bag of N-grams captures local word patterns.\",\n",
    "    \"TF-IDF considers term frequency and inverse document frequency.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_text.append(word)\n",
    "    \n",
    "    return \" \".join(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\naras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\naras\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing is an exciting field.', 'Preprocessing techniques are crucial for NLP tasks.', 'One Hot Encoding is a binary representation method.', 'Bag of Words is a common technique in NLP.', 'Bag of N-grams captures local word patterns.', 'TF-IDF considers term frequency and inverse document frequency.']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing is an exciting field.\n",
      "Preprocessing techniques are crucial for NLP tasks.\n",
      "One Hot Encoding is a binary representation method.\n",
      "Bag of Words is a common technique in NLP.\n",
      "Bag of N-grams captures local word patterns.\n",
      "TF-IDF considers term frequency and inverse document frequency.\n"
     ]
    }
   ],
   "source": [
    "for sentence in corpus:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'is', 'an', 'exciting', 'field']\n",
      "natural language processing exciting field\n",
      "['preprocessing', 'techniques', 'are', 'crucial', 'for', 'nlp', 'tasks']\n",
      "preprocessing techniques crucial nlp tasks\n",
      "['one', 'hot', 'encoding', 'is', 'a', 'binary', 'representation', 'method']\n",
      "one hot encoding binary representation method\n",
      "['bag', 'of', 'words', 'is', 'a', 'common', 'technique', 'in', 'nlp']\n",
      "bag words common technique nlp\n",
      "['bag', 'of', 'ngrams', 'captures', 'local', 'word', 'patterns']\n",
      "bag ngrams captures local word patterns\n",
      "['tfidf', 'considers', 'term', 'frequency', 'and', 'inverse', 'document', 'frequency']\n",
      "tfidf considers term frequency inverse document frequency\n"
     ]
    }
   ],
   "source": [
    "for sentence in corpus:\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
    "    # print(sentence)\n",
    "    words = word_tokenize(sentence)\n",
    "    print(words)\n",
    "    words=remove_stopwords(sentence)\n",
    "    print(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing is an exciting field\n",
      "/////\n",
      "natural language processing exciting field\n",
      "preprocessing techniques are crucial for nlp tasks\n",
      "/////\n",
      "preprocessing techniques crucial nlp tasks\n",
      "one hot encoding is a binary representation method\n",
      "/////\n",
      "one hot encoding binary representation method\n",
      "bag of words is a common technique in nlp\n",
      "/////\n",
      "bag words common technique nlp\n",
      "bag of ngrams captures local word patterns\n",
      "/////\n",
      "bag ngrams captures local word patterns\n",
      "tfidf considers term frequency and inverse document frequency\n",
      "/////\n",
      "tfidf considers term frequency inverse document frequency\n",
      "['natur languag process excit field', 'preprocess techniqu crucial nlp task', 'one hot encod binari represent method', 'bag word common techniqu nlp', 'bag ngram captur local word pattern', 'tfidf consid term frequenc invers document frequenc']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Preprocessing\n",
    "processed_corpus = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
    "    print(sentence)\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stop words\n",
    "    # words = [word for word in words if word not in stopwords.words('english')]\n",
    "    # sentence.str.apply(remove_stopwords)\n",
    "    words=remove_stopwords(sentence)\n",
    "    print(\"/////\")\n",
    "    print(words)\n",
    "    # Stemming\n",
    "# Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words.split()]\n",
    "\n",
    "    processed_corpus.append(' '.join(words))\n",
    "\n",
    "\n",
    "print(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing', 'exciting', 'field']\n",
      "['natur', 'languag', 'process', 'excit', 'field']\n",
      "natur languag process excit field\n",
      "['preprocessing', 'techniques', 'crucial', 'nlp', 'tasks']\n",
      "['preprocess', 'techniqu', 'crucial', 'nlp', 'task']\n",
      "preprocess techniqu crucial nlp task\n",
      "['one', 'hot', 'encoding', 'binary', 'representation', 'method']\n",
      "['one', 'hot', 'encod', 'binari', 'represent', 'method']\n",
      "one hot encod binari represent method\n",
      "['bag', 'words', 'common', 'technique', 'nlp']\n",
      "['bag', 'word', 'common', 'techniqu', 'nlp']\n",
      "bag word common techniqu nlp\n",
      "['bag', 'ngrams', 'captures', 'local', 'word', 'patterns']\n",
      "['bag', 'ngram', 'captur', 'local', 'word', 'pattern']\n",
      "bag ngram captur local word pattern\n",
      "['tfidf', 'considers', 'term', 'frequency', 'inverse', 'document', 'frequency']\n",
      "['tfidf', 'consid', 'term', 'frequenc', 'invers', 'document', 'frequenc']\n",
      "tfidf consid term frequenc invers document frequenc\n",
      "['natur languag process excit field', 'preprocess techniqu crucial nlp task', 'one hot encod binari represent method', 'bag word common techniqu nlp', 'bag ngram captur local word pattern', 'tfidf consid term frequenc invers document frequenc']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Preprocessing\n",
    "processed_corpus = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    print(words)\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    print(words)\n",
    "    # processed_corpus.append(' '.join(words))\n",
    "\n",
    "    joined_string = ' '.join(words)\n",
    "    print(joined_string)\n",
    "\n",
    "# Step 2: Append the joined string to the 'processed_corpus' list\n",
    "    processed_corpus.append(joined_string)\n",
    "\n",
    "\n",
    "print(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur languag process excit field', 'preprocess techniqu crucial nlp task', 'one hot encod binari represent method', 'bag word common techniqu nlp', 'bag ngram captur local word pattern', 'tfidf consid term frequenc invers document frequenc']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Sample dataset\n",
    "corpus = [\n",
    "    \"Natural Language Processing is an exciting field.\",\n",
    "    \"Preprocessing techniques are crucial for NLP tasks.\",\n",
    "    \"One Hot Encoding is a binary representation method.\",\n",
    "    \"Bag of Words is a common technique in NLP.\",\n",
    "    \"Bag of N-grams captures local word patterns.\",\n",
    "    \"TF-IDF considers term frequency and inverse document frequency.\"\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "processed_corpus = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    # Convert to lowercase\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stop words using a regular for loop\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    processed_corpus.append(' '.join(stemmed_words))\n",
    "\n",
    "print(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur languag process excit field', 'preprocess techniqu crucial nlp task', 'one hot encod binari represent method', 'bag word common techniqu nlp', 'bag ngram captur local word pattern', 'tfidf consid term frequenc invers document frequenc']\n",
      "['natur', 'languag', 'process', 'excit', 'field', 'preprocess', 'techniqu', 'crucial', 'nlp', 'task', 'one', 'hot', 'encod', 'binari', 'represent', 'method', 'bag', 'word', 'common', 'techniqu', 'nlp', 'bag', 'ngram', 'captur', 'local', 'word', 'pattern', 'tfidf', 'consid', 'term', 'frequenc', 'invers', 'document', 'frequenc']\n",
      "Total Words: 34\n",
      "Vocabulary Size: 29\n"
     ]
    }
   ],
   "source": [
    "print(processed_corpus)\n",
    "flat_list = [word for sentence in processed_corpus for word in sentence.split()]\n",
    "print(flat_list)\n",
    "total_words = len(flat_list)\n",
    "unique_words = set(flat_list)\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "print(\"Total Words:\", total_words)\n",
    "print(\"Vocabulary Size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "{'natur': 16, 'languag': 13, 'process': 22, 'excit': 8, 'field': 9, 'preprocess': 21, 'techniqu': 25, 'crucial': 5, 'nlp': 18, 'task': 24, 'one': 19, 'hot': 11, 'encod': 7, 'binari': 1, 'represent': 23, 'method': 15, 'bag': 0, 'word': 28, 'common': 3, 'ngram': 17, 'captur': 2, 'local': 14, 'pattern': 20, 'tfidf': 27, 'consid': 4, 'term': 26, 'frequenc': 10, 'invers': 12, 'document': 6}\n",
      "['bag' 'binari' 'captur' 'common' 'consid' 'crucial' 'document' 'encod'\n",
      " 'excit' 'field' 'frequenc' 'hot' 'invers' 'languag' 'local' 'method'\n",
      " 'natur' 'ngram' 'nlp' 'one' 'pattern' 'preprocess' 'process' 'represent'\n",
      " 'task' 'techniqu' 'term' 'tfidf' 'word']\n",
      "29\n",
      "Word Occurrences:\n",
      "[[0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 1 0 0 0 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0]]\n",
      "(6, 29)\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(len(vectorizer.get_feature_names_out()))\n",
    "print(\"Word Occurrences:\")\n",
    "print(bag_of_words.toarray())\n",
    "print(bag_of_words.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['bag' 'binary' 'captures' 'common' 'considers' 'crucial' 'document'\n",
      " 'encoding' 'exciting' 'field' 'frequency' 'grams' 'hot' 'idf' 'inverse'\n",
      " 'language' 'local' 'method' 'natural' 'nlp' 'one' 'patterns'\n",
      " 'preprocessing' 'processing' 'representation' 'tasks' 'technique'\n",
      " 'techniques' 'term' 'tf' 'word' 'words']\n",
      "Word Occurrences:\n",
      "[[0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1]\n",
      " [1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 1 0 0 0 2 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]]\n",
      "  (0, 18)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 27)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 25)\t1\n",
      "  (2, 20)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 24)\t1\n",
      "  (2, 17)\t1\n",
      "  (3, 19)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 31)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 26)\t1\n",
      "  (4, 0)\t1\n",
      "  (4, 11)\t1\n",
      "  (4, 2)\t1\n",
      "  (4, 16)\t1\n",
      "  (4, 30)\t1\n",
      "  (4, 21)\t1\n",
      "  (5, 29)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 4)\t1\n",
      "  (5, 28)\t1\n",
      "  (5, 10)\t2\n",
      "  (5, 14)\t1\n",
      "  (5, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample processed_corpus\n",
    "processed_corpus = [\n",
    "    'natural language processing exciting field',\n",
    "    'preprocessing techniques crucial nlp tasks',\n",
    "    'one hot encoding binary representation method',\n",
    "    'bag words common technique nlp',\n",
    "    'bag n-grams captures local word patterns',\n",
    "    'tf-idf considers term frequency inverse document frequency'\n",
    "]\n",
    "\n",
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"Word Occurrences:\")\n",
    "print(bag_of_words.toarray())\n",
    "print(bag_of_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram Vocabulary:\n",
      "{'natural language': 17, 'language processing': 15, 'processing exciting': 21, 'exciting field': 9, 'preprocessing techniques': 20, 'techniques crucial': 24, 'crucial nlp': 6, 'nlp tasks': 18, 'one hot': 19, 'hot encoding': 12, 'encoding binary': 8, 'binary representation': 2, 'representation method': 22, 'bag words': 1, 'words common': 28, 'common technique': 4, 'technique nlp': 23, 'bag grams': 0, 'grams captures': 11, 'captures local': 3, 'local word': 16, 'word patterns': 27, 'tf idf': 26, 'idf considers': 13, 'considers term': 5, 'term frequency': 25, 'frequency inverse': 10, 'inverse document': 14, 'document frequency': 7}\n",
      "['bag grams' 'bag words' 'binary representation' 'captures local'\n",
      " 'common technique' 'considers term' 'crucial nlp' 'document frequency'\n",
      " 'encoding binary' 'exciting field' 'frequency inverse' 'grams captures'\n",
      " 'hot encoding' 'idf considers' 'inverse document' 'language processing'\n",
      " 'local word' 'natural language' 'nlp tasks' 'one hot'\n",
      " 'preprocessing techniques' 'processing exciting' 'representation method'\n",
      " 'technique nlp' 'techniques crucial' 'term frequency' 'tf idf'\n",
      " 'word patterns' 'words common']\n",
      "Tri-gram Vocabulary:\n",
      "['bag grams captures' 'bag words common' 'binary representation method'\n",
      " 'captures local word' 'common technique nlp' 'considers term frequency'\n",
      " 'crucial nlp tasks' 'encoding binary representation'\n",
      " 'frequency inverse document' 'grams captures local' 'hot encoding binary'\n",
      " 'idf considers term' 'inverse document frequency'\n",
      " 'language processing exciting' 'local word patterns'\n",
      " 'natural language processing' 'one hot encoding'\n",
      " 'preprocessing techniques crucial' 'processing exciting field'\n",
      " 'techniques crucial nlp' 'term frequency inverse' 'tf idf considers'\n",
      " 'words common technique']\n"
     ]
    }
   ],
   "source": [
    "# Bag of Bi-gram and Bag of Tri-gram\n",
    "bi_gram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "tri_gram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "bi_grams = bi_gram_vectorizer.fit_transform(processed_corpus)\n",
    "tri_grams = tri_gram_vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "print(\"Bi-gram Vocabulary:\")\n",
    "print(bi_gram_vectorizer.vocabulary_)\n",
    "print(bi_gram_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Tri-gram Vocabulary:\")\n",
    "print(tri_gram_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vocabulary:\n",
      "['bag' 'binary' 'captures' 'common' 'considers' 'crucial' 'document'\n",
      " 'encoding' 'exciting' 'field' 'frequency' 'grams' 'hot' 'idf' 'inverse'\n",
      " 'language' 'local' 'method' 'natural' 'nlp' 'one' 'patterns'\n",
      " 'preprocessing' 'processing' 'representation' 'tasks' 'technique'\n",
      " 'techniques' 'term' 'tf' 'word' 'words']\n",
      "IDF Scores:\n",
      "[1.84729786 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 1.84729786 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297 2.25276297\n",
      " 2.25276297 2.25276297]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "print(\"TF-IDF Vocabulary:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"IDF Scores:\")\n",
    "print(tfidf_vectorizer.idf_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
